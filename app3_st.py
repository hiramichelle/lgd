import streamlit as st
import pandas as pd
import logging
import requests
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import matplotlib.ticker as ticker
import matplotlib.dates as mdates
import re # æ­£è¦è¡¨ç¾ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# --- æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š ---
try:
    plt.rcParams['axes.unicode_minus'] = False
    st.info("â€»æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã¯ãƒ‡ãƒ—ãƒ­ã‚¤ç’°å¢ƒã§åˆ©ç”¨ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã‚°ãƒ©ãƒ•ã®æ—¥æœ¬èªãŒæ–‡å­—åŒ–ã‘ã™ã‚‹å ´åˆã¯ã”å®¹èµ¦ãã ã•ã„ã€‚")
except Exception as e:
    st.warning(f"ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    plt.rcParams['axes.unicode_minus'] = False

# --- ãƒ­ã‚°è¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
)

logging.info("--- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ ---")

# --------------------------------------------------------------------------
# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°: ãƒªãƒ¼ã‚°åãƒ»ãƒãƒ¼ãƒ åã‚’æ­£è¦åŒ–ã™ã‚‹
# --------------------------------------------------------------------------
def normalize_j_name(name):
    """Jãƒªãƒ¼ã‚°åã‚„ãƒãƒ¼ãƒ åã‚’åŠè§’ã«çµ±ä¸€ã™ã‚‹"""
    if isinstance(name, str):
        # å…¨è§’è‹±æ•°å­—ã‚’åŠè§’ã«ã€å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã«
        normalized = name.translate(str.maketrans('ï¼‘ï¼’ï¼“', '123')).replace('ã€€', ' ').strip()
        # Jãƒªãƒ¼ã‚°åã®å…¨è§’ã‚’åŠè§’ã«
        normalized = normalized.replace('ï¼ª', 'J')
        # ãã®ä»–ã®ç‰¹å®šã®å…¨è§’æ–‡å­—ï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰ã‚’åŠè§’ã«å¤‰æ›ã™ã‚‹ä¾‹
        # ä¾‹: ãƒãƒ¼ãƒ åã«ã€ŒFCæ±äº¬ã€ã‚„ã€ŒæŸãƒ¬ã‚¤ã‚½ãƒ«ã€ãªã©ã®æºã‚ŒãŒã‚ã‚‹å ´åˆã€åˆ¥é€”å‡¦ç†ãŒå¿…è¦
        return normalized
    return name

# --------------------------------------------------------------------------
# Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°
# --------------------------------------------------------------------------
@st.cache_data(ttl=3600) # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def scrape_ranking_data(url):
    """
    Jãƒªãƒ¼ã‚°å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰é †ä½è¡¨ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹é–¢æ•°ã€‚
    """
    logging.info(f"scrape_ranking_data: URL {url} ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ã€‚")
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        dfs = pd.read_html(response.text, flavor='lxml', header=0, match='é †ä½')
        
        if not dfs:
            logging.warning("read_htmlãŒãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚URL: %s", url)
            return None
        df = dfs[0]
        logging.info(f"é †ä½è¡¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆåŠŸã€‚DataFrameã®å½¢çŠ¶: {df.shape}")
        if 'å‚™è€ƒ' in df.columns:
            df = df.drop(columns=['å‚™è€ƒ'])
        
        # é †ä½è¡¨ãƒ‡ãƒ¼ã‚¿ã«ã‚‚ãƒªãƒ¼ã‚°åã‚’æ­£è¦åŒ–ã—ã¦è¿½åŠ ï¼ˆJ1, J2, J3 ãªã©ï¼‰
        # URLã‹ã‚‰ãƒªãƒ¼ã‚°åã‚’æŠ½å‡ºã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã“ã“ã«æŒ¿å…¥ã™ã‚‹ã‹ã€å‘¼ã³å‡ºã—å…ƒã§è¨­å®š
        return df
    except requests.exceptions.HTTPError as errh:
        logging.error(f"HTTPã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {errh}")
        st.error(f"é †ä½è¡¨ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: HTTPã‚¨ãƒ©ãƒ¼ {errh.response.status_code}")
        return None
    except requests.exceptions.ConnectionError as errc:
        logging.error(f"æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {errc}")
        st.error(f"é †ä½è¡¨ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return None
    except requests.exceptions.Timeout as errt:
        logging.error(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {errt}")
        st.error(f"é †ä½è¡¨ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚")
        return None
    except requests.exceptions.RequestException as err:
        logging.error(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {err}")
        st.error(f"é †ä½è¡¨ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: ä¸æ˜ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ã€‚")
        return None
    except Exception as e:
        logging.error(f"é †ä½è¡¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
        st.error(f"é †ä½è¡¨ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

@st.cache_data(ttl=3600) # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def scrape_schedule_data(url):
    """
    Jãƒªãƒ¼ã‚°å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰æ—¥ç¨‹è¡¨ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹é–¢æ•°ã€‚
    """
    logging.info(f"scrape_schedule_data: URL {url} ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ã€‚")
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        dfs = pd.read_html(response.text, flavor='lxml', header=0, match='è©¦åˆæ—¥')
        
        if not dfs:
            logging.warning("read_htmlãŒãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚URL: %s", url)
            return None
            
        df = dfs[0]
        logging.info(f"æ—¥ç¨‹è¡¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆåŠŸã€‚DataFrameã®å½¢çŠ¶: {df.shape}, ã‚«ãƒ©ãƒ æ•°: {len(df.columns)}")
        
        expected_cols = ['å¤§ä¼š', 'è©¦åˆæ—¥', 'ã‚­ãƒƒã‚¯ã‚ªãƒ•', 'ã‚¹ã‚¿ã‚¸ã‚¢ãƒ ', 'ãƒ›ãƒ¼ãƒ ', 'ã‚¹ã‚³ã‚¢', 'ã‚¢ã‚¦ã‚§ã‚¤', 'ãƒ†ãƒ¬ãƒ“ä¸­ç¶™']
        
        cols_to_keep = [col for col in expected_cols if col in df.columns]
        
        if not cols_to_keep:
            logging.error("æŠ½å‡ºã§ããŸåˆ—ãŒä¸€ã¤ã‚‚ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒˆã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒå¤§å¹…ã«å¤‰æ›´ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            st.error("æ—¥ç¨‹è¡¨ã®åˆ—æƒ…å ±ãŒæƒ³å®šã¨ç•°ãªã‚Šã¾ã™ã€‚ã‚µã‚¤ãƒˆã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
            return None
            
        df = df[cols_to_keep]

        # å¤§ä¼šåã¨ãƒãƒ¼ãƒ åã‚’æ­£è¦åŒ–
        if 'å¤§ä¼š' in df.columns:
            df['å¤§ä¼š'] = df['å¤§ä¼š'].apply(normalize_j_name)
        if 'ãƒ›ãƒ¼ãƒ ' in df.columns:
            df['ãƒ›ãƒ¼ãƒ '] = df['ãƒ›ãƒ¼ãƒ '].apply(normalize_j_name)
        if 'ã‚¢ã‚¦ã‚§ã‚¤' in df.columns:
            df['ã‚¢ã‚¦ã‚§ã‚¤'] = df['ã‚¢ã‚¦ã‚§ã‚¤'].apply(normalize_j_name)

        return df
        
    except requests.exceptions.HTTPError as errh:
        logging.error(f"HTTPã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {errh}")
        st.error(f"æ—¥ç¨‹è¡¨ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: HTTPã‚¨ãƒ©ãƒ¼ {errh.response.status_code}")
        return None
    except requests.exceptions.ConnectionError as errc:
        logging.error(f"æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {errc}")
        st.error(f"æ—¥ç¨‹è¡¨ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return None
    except requests.exceptions.Timeout as errt:
        logging.error(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {errt}")
        st.error(f"æ—¥ç¨‹è¡¨ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚")
        return None
    except requests.exceptions.RequestException as err:
        logging.error(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {err}")
        st.error(f"æ—¥ç¨‹è¡¨ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: ä¸æ˜ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ã€‚")
        return None
    except Exception as e:
        logging.error(f"æ—¥ç¨‹è¡¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
        st.error(f"æ—¥ç¨‹è¡¨ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# --------------------------------------------------------------------------
# ãƒ‡ãƒ¼ã‚¿åŠ å·¥é–¢æ•°
# --------------------------------------------------------------------------
@st.cache_data(ttl=3600) # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def create_point_aggregate_df(schedule_df):
    """æ—¥ç¨‹è¡¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€ãƒãƒ¼ãƒ ã”ã¨ã®è©¦åˆçµæœã‚’é›†è¨ˆã™ã‚‹DataFrameã‚’ä½œæˆ"""
    if schedule_df is None or schedule_df.empty:
        logging.info("create_point_aggregate_df: å…¥åŠ›schedule_dfãŒNoneã¾ãŸã¯ç©ºã§ã™ã€‚")
        return pd.DataFrame()

    df = schedule_df.copy()
    logging.info(f"create_point_aggregate_df: å…ƒã®schedule_dfã®è¡Œæ•°: {len(df)}")

    # ã‚¹ã‚³ã‚¢ãŒã€Œæ•°å­—-æ•°å­—ã€ã®å½¢å¼ã§ãªã„è¡Œã‚’é™¤å¤– (r'' ã§ SyntaxWarning ã‚’å›é¿)
    # 2025å¹´ãªã©ã®æœªé–‹å‚¬è©¦åˆã§ã¯ã€Œ-ã€ã®ã¿ã€ã¾ãŸã¯ç©ºã®å ´åˆãŒã‚ã‚‹ãŸã‚ã€ã“ã‚Œã‚‰ã‚’å¼¾ã
    initial_rows = len(df)
    df = df[df['ã‚¹ã‚³ã‚¢'].str.contains(r'^\d+-\d+$', na=False)]
    logging.info(f"create_point_aggregate_df: ã‚¹ã‚³ã‚¢ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®è¡Œæ•°: {len(df)} (é™¤å¤–: {initial_rows - len(df)})")

    if df.empty:
        logging.info("create_point_aggregate_df: ã‚¹ã‚³ã‚¢å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return pd.DataFrame()
    
    df[['å¾—ç‚¹H', 'å¾—ç‚¹A']] = df['ã‚¹ã‚³ã‚¢'].str.split('-', expand=True).astype(int)

    # è©¦åˆæ—¥ã®å‰å‡¦ç†
    # '24/02/24(åœŸ)' -> '2024/02/24'
    # 2025å¹´ã®ã‚µã‚¤ãƒˆã®æ—¥ä»˜å½¢å¼ã«æŸ”è»Ÿã«å¯¾å¿œã™ã‚‹ãŸã‚ã€è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
    initial_rows = len(df)
    
    # æ‹¬å¼§å†…ã®æ›œæ—¥æƒ…å ±ã‚’å‰Šé™¤
    df['è©¦åˆæ—¥'] = df['è©¦åˆæ—¥'].str.replace(r'\(.+\)', '', regex=True)
    
    # å¹´ãŒçœç•¥ã•ã‚Œã¦ã„ã‚‹å ´åˆï¼ˆä¾‹: '02/24'ï¼‰ã«ç¾åœ¨ã®å¹´åº¦ã‚’ä»˜ä¸
    # 'YY/MM/DD' å½¢å¼ã‚’å‰æã¨ã™ã‚‹ãŒã€'MM/DD' ã®å¯èƒ½æ€§ã‚‚è€ƒæ…®
    def parse_match_date(date_str, current_year):
        if pd.isna(date_str) or not isinstance(date_str, str):
            return pd.NaT # Not a Time
        
        # 'YY/MM/DD' ã¾ãŸã¯ 'YYYY/MM/DD' ã®å½¢å¼ã‚’è©¦ã™
        try:
            # 'YY/MM/DD' ã®å½¢å¼ã‚’ã¾ãšè©¦ã™ (ä¾‹: '25/02/24')
            return pd.to_datetime(date_str, format='%y/%m/%d')
        except ValueError:
            try:
                # 'YYYY/MM/DD' ã®å½¢å¼ã‚’è©¦ã™ (ä¾‹: '2025/02/24')
                return pd.to_datetime(date_str, format='%Y/%m/%d')
            except ValueError:
                # 'MM/DD' ã®å½¢å¼ã®å ´åˆã€ç¾åœ¨ã®å¹´åº¦ã‚’ä»˜ä¸ã—ã¦è©¦ã™ (ä¾‹: '02/24')
                try:
                    return pd.to_datetime(f'{current_year}/{date_str}', format='%Y/%m/%d')
                except ValueError:
                    return pd.NaT
    
    df['è©¦åˆæ—¥'] = df['è©¦åˆæ—¥'].apply(lambda x: parse_match_date(x, st.session_state.current_year)) # selectboxã§é¸æŠã•ã‚ŒãŸcurrent_yearã‚’ä½¿ç”¨
    
    # ç„¡åŠ¹ãªæ—¥ä»˜ï¼ˆNaTï¼‰ã®è¡Œã‚’å‰Šé™¤
    df.dropna(subset=['è©¦åˆæ—¥'], inplace=True)
    logging.info(f"create_point_aggregate_df: æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹å¾Œã®è¡Œæ•°: {len(df)} (é™¤å¤–: {initial_rows - len(df)})")

    if df.empty:
        logging.info("create_point_aggregate_df: æ—¥ä»˜ãŒæœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return pd.DataFrame()

    # ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ã®ãƒ‡ãƒ¼ã‚¿é›†è¨ˆ
    home_df = df.rename(columns={'ãƒ›ãƒ¼ãƒ ': 'ãƒãƒ¼ãƒ ', 'ã‚¢ã‚¦ã‚§ã‚¤': 'ç›¸æ‰‹', 'å¾—ç‚¹H': 'å¾—ç‚¹', 'å¾—ç‚¹A': 'å¤±ç‚¹'})
    home_df['å¾—å¤±å·®'] = home_df['å¾—ç‚¹'] - home_df['å¤±ç‚¹']
    home_df['å‹æ•—'] = home_df.apply(lambda row: 'å‹' if row['å¾—ç‚¹'] > row['å¤±ç‚¹'] else ('åˆ†' if row['å¾—ç‚¹'] == row['å¤±ç‚¹'] else 'æ•—'), axis=1)
    home_df['å‹ç‚¹'] = home_df.apply(lambda row: 3 if row['å‹æ•—'] == 'å‹' else (1 if row['å‹æ•—'] == 'åˆ†' else 0), axis=1)
    home_df['å¯¾æˆ¦ç›¸æ‰‹'] = home_df['ç›¸æ‰‹']
    home_df = home_df[['å¤§ä¼š', 'è©¦åˆæ—¥', 'ãƒãƒ¼ãƒ ', 'å¯¾æˆ¦ç›¸æ‰‹', 'å‹æ•—', 'å¾—ç‚¹', 'å¤±ç‚¹', 'å¾—å¤±å·®', 'å‹ç‚¹']]

    # ã‚¢ã‚¦ã‚§ã‚¤ãƒãƒ¼ãƒ ã®ãƒ‡ãƒ¼ã‚¿é›†è¨ˆ
    away_df = df.rename(columns={'ã‚¢ã‚¦ã‚§ã‚¤': 'ãƒãƒ¼ãƒ ', 'ãƒ›ãƒ¼ãƒ ': 'ç›¸æ‰‹', 'å¾—ç‚¹A': 'å¾—ç‚¹', 'å¾—ç‚¹H': 'å¤±ç‚¹'})
    away_df['å¾—å¤±å·®'] = away_df['å¾—ç‚¹'] - away_df['å¤±ç‚¹']
    away_df['å‹æ•—'] = away_df.apply(lambda row: 'å‹' if row['å¾—ç‚¹'] > row['å¤±ç‚¹'] else ('åˆ†' if row['å¾—ç‚¹'] == row['å¤±ç‚¹'] else 'æ•—'), axis=1)
    away_df['å‹ç‚¹'] = away_df.apply(lambda row: 3 if row['å‹æ•—'] == 'å‹' else (1 if row['å‹æ•—'] == 'åˆ†' else 0), axis=1)
    away_df['å¯¾æˆ¦ç›¸æ‰‹'] = away_df['ç›¸æ‰‹']
    away_df = away_df[['å¤§ä¼š', 'è©¦åˆæ—¥', 'ãƒãƒ¼ãƒ ', 'å¯¾æˆ¦ç›¸æ‰‹', 'å‹æ•—', 'å¾—ç‚¹', 'å¤±ç‚¹', 'å¾—å¤±å·®', 'å‹ç‚¹']]

    # ãƒ›ãƒ¼ãƒ ã¨ã‚¢ã‚¦ã‚§ã‚¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    pointaggregate_df = pd.concat([home_df, away_df], ignore_index=True)
    logging.info(f"create_point_aggregate_df: çµåˆå¾Œã®ç·è©¦åˆãƒ‡ãƒ¼ã‚¿è¡Œæ•°: {len(pointaggregate_df)}")

    # è©¦åˆæ—¥ã§ã‚½ãƒ¼ãƒˆã—ã€ç´¯ç©å‹ç‚¹ã‚’è¨ˆç®—
    pointaggregate_df = pointaggregate_df.sort_values(by=['è©¦åˆæ—¥'], ascending=True)
    pointaggregate_df['ç´¯ç©å‹ç‚¹'] = pointaggregate_df.groupby(['ãƒãƒ¼ãƒ '])['å‹ç‚¹'].cumsum()
    logging.info(f"create_point_aggregate_df: ç´¯ç©å‹ç‚¹è¨ˆç®—å¾Œã®æœ€çµ‚è¡Œæ•°: {len(pointaggregate_df)}")

    return pointaggregate_df


# --------------------------------------------------------------------------
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æœ¬ä½“
# --------------------------------------------------------------------------
try:
    st.title('ğŸ“Š Jãƒªãƒ¼ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ“ãƒ¥ãƒ¼ã‚¢')

    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§å¹´åº¦ã‚’é¸æŠã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ ---
    with st.sidebar:
        st.header("ãƒ‡ãƒ¼ã‚¿å¹´åº¦é¸æŠ")
        years = list(range(2020, pd.Timestamp.now().year + 2))
        selected_year = st.selectbox("è¡¨ç¤ºã™ã‚‹å¹´åº¦ã‚’é¸æŠã—ã¦ãã ã•ã„:", years, index=years.index(pd.Timestamp.now().year), key='year_selector')
        # Streamlit Session State ã«é¸æŠã•ã‚ŒãŸå¹´åº¦ã‚’ä¿å­˜
        st.session_state.current_year = selected_year


    # --- ãƒ‡ãƒ¼ã‚¿ã®å–å¾— ---
    # é †ä½è¡¨URLã¯ã€ç¾åœ¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€
    # å®Ÿéš›ã«ã¯ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã„å¯èƒ½æ€§ãŒé«˜ã„ã“ã¨ã‚’è€ƒæ…®ã—ã¦æ§‹ç¯‰
    ranking_urls = {
        'J1': f'https://data.j-league.or.jp/SFRT01/?competitionSectionIdLabel=%E6%9C%80%E6%96%B0%E7%AF%80&competitionIdLabel=%E6%98%8E%E6%B2%BB%E7%94%B0%EF%BC%AA%EF%BC%91%E3%83%AA%E3%83%BC%E3%82%B0&yearIdLabel={st.session_state.current_year}&yearId={st.session_state.current_year}&competitionId=651&competitionSectionId=0&search=search',
        'J2': f'https://data.j-league.or.jp/SFRT01/?competitionSectionIdLabel=%E6%9C%80%E6%96%B0%E7%AF%80&competitionIdLabel=%E6%98%8E%E6%B2%BB%E7%94%B0%EF%BC%AA%EF%BC%92%E3%83%AA%E3%83%BC%E3%82%B0&yearIdLabel={st.session_state.current_year}&yearId={st.session_state.current_year}&competitionId=655&competitionSectionId=0&search=search',
        'J3': f'https://data.j-league.or.jp/SFRT01/?competitionSectionIdLabel=%E6%9C%80%E6%96%B0%E7%AF%80&competitionIdLabel=%E6%98%8E%E6%B2%BB%E7%94%B0%EF%BC%AA%EF%BC%93%E3%83%AA%E3%83%BC%E3%82%B0&yearIdLabel={st.session_state.current_year}&yearId={st.session_state.current_year}&competitionId=657&competitionSectionId=0&search=search'
    }
    schedule_url = f'https://data.j-league.or.jp/SFMS01/search?competition_years={st.session_state.current_year}&competition_frame_ids=1&competition_frame_ids=2&competition_frame_ids=3&tv_relay_station_name='

    ranking_dfs_raw = {league: scrape_ranking_data(url) for league, url in ranking_urls.items()}
    
    combined_ranking_df = pd.DataFrame()
    ranking_data_available = False
    
    valid_ranking_dfs = [df for df in ranking_dfs_raw.values() if df is not None]
    if valid_ranking_dfs:
        try:
            for league, df_val in ranking_dfs_raw.items(): # df_valã¨ã„ã†å¤‰æ•°åã«å¤‰æ›´
                if df_val is not None:
                    df_val['å¤§ä¼š'] = normalize_j_name(league) 
            combined_ranking_df = pd.concat(valid_ranking_dfs, ignore_index=True)
            ranking_data_available = True
        except ValueError as e:
            logging.error(f"é †ä½è¡¨ãƒ‡ãƒ¼ã‚¿çµåˆã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            st.error("é †ä½è¡¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    
    if not ranking_data_available:
        st.warning("ç¾åœ¨ã€Jãƒªãƒ¼ã‚°å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰ã®é †ä½è¡¨ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å•é¡ŒãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚æ™‚é–“ã‚’ç½®ã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")

    schedule_df = scrape_schedule_data(schedule_url) # æ—¥ç¨‹è¡¨ãƒ‡ãƒ¼ã‚¿ã¯ã“ã“ã§å–å¾—ã—ã€å†…éƒ¨ã§æ­£è¦åŒ–æ¸ˆã¿

    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«é¸æŠè‚¢ã‚’é…ç½® ---
    with st.sidebar:
        st.header("è¡¨ç¤ºãƒ‡ãƒ¼ã‚¿é¸æŠ")
        
        data_type_options = ["æ—¥ç¨‹è¡¨"] # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æ—¥ç¨‹è¡¨ã®ã¿
        if schedule_df is not None and not schedule_df.empty:
            # æ—¥ç¨‹è¡¨ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°ã€ãã®å¾Œã®é›†è¨ˆã‚‚å¯èƒ½ã‹ã‚‚ã—ã‚Œãªã„ã®ã§è¿½åŠ 
            data_type_options.extend(["ç›´è¿‘5è©¦åˆ", "é †ä½å¤‰å‹•ã‚°ãƒ©ãƒ•"])
        
        if ranking_data_available and not combined_ranking_df.empty: 
             data_type_options.insert(0, "é †ä½è¡¨")
        
        data_type = st.radio("è¡¨ç¤ºã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠã—ã¦ãã ã•ã„:", data_type_options)

    # --- ãƒ¡ã‚¤ãƒ³ç”»é¢ã®è¡¨ç¤ºãƒ­ã‚¸ãƒƒã‚¯ ---
    if data_type == "é †ä½è¡¨":
        st.header(f"Jãƒªãƒ¼ã‚° {st.session_state.current_year} é †ä½è¡¨")
        if ranking_data_available and not combined_ranking_df.empty:
            with st.sidebar:
                st.header("é †ä½è¡¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
                league_options = combined_ranking_df['å¤§ä¼š'].unique()
                if not list(league_options): # ãƒªãƒ¼ã‚°ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒç©ºã®å ´åˆã®ã‚¬ãƒ¼ãƒ‰
                    st.warning("é †ä½è¡¨ãƒ‡ãƒ¼ã‚¿ã«å¤§ä¼šæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                    st.stop()
                selected_league = st.selectbox('è¡¨ç¤ºã—ãŸã„å¤§ä¼šã‚’é¸æŠã—ã¦ãã ã•ã„:', league_options, key='ranking_selectbox')
            
            filtered_df = combined_ranking_df[combined_ranking_df['å¤§ä¼š'] == selected_league].drop(columns=['å¤§ä¼š'])
            st.dataframe(filtered_df)
        else:
            st.error("é †ä½è¡¨ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")

    elif data_type == "æ—¥ç¨‹è¡¨":
        st.header(f"Jãƒªãƒ¼ã‚° {st.session_state.current_year} è©¦åˆæ—¥ç¨‹")
        if schedule_df is not None and not schedule_df.empty:
            with st.sidebar:
                st.header("æ—¥ç¨‹è¡¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
                
                league_options = sorted(schedule_df['å¤§ä¼š'].unique())
                if not league_options:
                    st.warning("æ—¥ç¨‹è¡¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¤§ä¼šæƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                    st.stop()
                
                selected_league_schedule = st.selectbox('è¡¨ç¤ºã—ãŸã„å¤§ä¼šã‚’é¸æŠã—ã¦ãã ã•ã„:', league_options, key='schedule_league_selectbox')
                filtered_by_league = schedule_df[schedule_df['å¤§ä¼š'] == selected_league_schedule]
                
                all_teams_in_league = pd.concat([filtered_by_league['ãƒ›ãƒ¼ãƒ '], filtered_by_league['ã‚¢ã‚¦ã‚§ã‚¤']]).unique()
                team_options = sorted(all_teams_in_league)
                if not team_options: # ãƒãƒ¼ãƒ ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒç©ºã®å ´åˆã®ã‚¬ãƒ¼ãƒ‰
                    st.warning(f"é¸æŠã•ã‚ŒãŸå¤§ä¼š ({selected_league_schedule}) ã®ãƒãƒ¼ãƒ æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                    st.stop()

                selected_team = st.selectbox('è¡¨ç¤ºã—ãŸã„ãƒãƒ¼ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„:', team_options, key='schedule_team_selectbox')
            
            team_filter = (schedule_df['ãƒ›ãƒ¼ãƒ '] == selected_team) | (schedule_df['ã‚¢ã‚¦ã‚§ã‚¤'] == selected_team)
            final_filtered_df = schedule_df[(schedule_df['å¤§ä¼š'] == selected_league_schedule) & team_filter]
            st.dataframe(final_filtered_df)
        else:
            st.error("æ—¥ç¨‹è¡¨ãƒ‡ãƒ¼ã‚¿ãŒæ­£å¸¸ã«å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    elif data_type == "ç›´è¿‘5è©¦åˆ":
        st.header(f"{st.session_state.current_year} ãƒãƒ¼ãƒ åˆ¥ ç›´è¿‘5è©¦åˆçµæœ")
        pointaggregate_df = create_point_aggregate_df(schedule_df)
        if not pointaggregate_df.empty:
            with st.sidebar:
                st.header("ç›´è¿‘5è©¦åˆã‚ªãƒ—ã‚·ãƒ§ãƒ³")
                
                league_options_aggregate = sorted(pointaggregate_df['å¤§ä¼š'].unique())
                if not league_options_aggregate:
                    st.warning("é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¤§ä¼šæƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                    st.stop()
                    
                selected_league_aggregate = st.selectbox('å¤§ä¼šã‚’é¸æŠã—ã¦ãã ã•ã„:', league_options_aggregate, key='aggregate_league_selectbox')

                filtered_df_aggregate = pointaggregate_df[pointaggregate_df['å¤§ä¼š'] == selected_league_aggregate]
                team_options_aggregate = sorted(filtered_df_aggregate['ãƒãƒ¼ãƒ '].unique())
                if not team_options_aggregate: # ãƒãƒ¼ãƒ ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒç©ºã®å ´åˆã®ã‚¬ãƒ¼ãƒ‰
                    st.warning(f"é¸æŠã•ã‚ŒãŸå¤§ä¼š ({selected_league_aggregate}) ã®ãƒãƒ¼ãƒ æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                    st.stop()
                
                selected_team_aggregate = st.selectbox('ãƒãƒ¼ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„:', team_options_aggregate, key='aggregate_team_selectbox')
            
            team_results = pointaggregate_df[(pointaggregate_df['å¤§ä¼š'] == selected_league_aggregate) & (pointaggregate_df['ãƒãƒ¼ãƒ '] == selected_team_aggregate)]
            recent_5_games = team_results.sort_values(by='è©¦åˆæ—¥', ascending=False).head(5)
            recent_5_games = recent_5_games.sort_values(by='è©¦åˆæ—¥', ascending=True)
            
            recent_5_games['è©¦åˆæ—¥'] = recent_5_games['è©¦åˆæ—¥'].dt.strftime('%y%m%d')
            
            st.dataframe(recent_5_games)
        else:
            st.error("æ—¥ç¨‹è¡¨ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€ç›´è¿‘5è©¦åˆã®é›†è¨ˆãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    elif data_type == "é †ä½å¤‰å‹•ã‚°ãƒ©ãƒ•":
        st.header(f"{st.session_state.current_year} ãƒãƒ¼ãƒ åˆ¥ é †ä½å¤‰å‹•ã‚°ãƒ©ãƒ•")
        pointaggregate_df = create_point_aggregate_df(schedule_df)
        if not pointaggregate_df.empty:
            with st.sidebar:
                st.header("é †ä½å¤‰å‹•ã‚°ãƒ©ãƒ•ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
                
                league_options_rank = sorted(pointaggregate_df['å¤§ä¼š'].unique())
                if not league_options_rank:
                    st.warning("é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¤§ä¼šæƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                    st.stop()
                    
                selected_league_rank = st.selectbox('å¤§ä¼šã‚’é¸æŠã—ã¦ãã ã•ã„:', league_options_rank, key='rank_league_selectbox')

                filtered_df_rank = pointaggregate_df[pointaggregate_df['å¤§ä¼š'] == selected_league_rank]
                team_options_rank = sorted(filtered_df_rank['ãƒãƒ¼ãƒ '].unique())
                if not team_options_rank: # ãƒãƒ¼ãƒ ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒç©ºã®å ´åˆã®ã‚¬ãƒ¼ãƒ‰
                    st.warning(f"é¸æŠã•ã‚ŒãŸå¤§ä¼š ({selected_league_rank}) ã®ãƒãƒ¼ãƒ æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                    st.stop()
                
                selected_teams_rank = st.multiselect('ãƒãƒ¼ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„ (è¤‡æ•°é¸æŠå¯):', team_options_rank, default=team_options_rank[:1], key='rank_team_multiselect')
            
            if not selected_teams_rank:
                st.warning("è¡¨ç¤ºã™ã‚‹ãƒãƒ¼ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            else:
                min_date = filtered_df_rank['è©¦åˆæ—¥'].min()
                max_date = filtered_df_rank['è©¦åˆæ—¥'].max()
                
                start_monday_candidate = min_date - pd.to_timedelta(min_date.weekday(), unit='D')
                if start_monday_candidate < min_date:
                    start_monday = start_monday_candidate + pd.to_timedelta(7, unit='D')
                else:
                    start_monday = start_monday_candidate
                
                weekly_mondays = pd.date_range(start=start_monday, end=max_date + pd.to_timedelta(7, unit='D'), freq='W-MON')
                
                all_teams_in_selected_league = filtered_df_rank['ãƒãƒ¼ãƒ '].unique()
                
                weekly_rank_data = pd.DataFrame(index=weekly_mondays)

                for team in all_teams_in_selected_league: 
                    team_cumulative_points = filtered_df_rank[
                        filtered_df_rank['ãƒãƒ¼ãƒ '] == team
                    ].set_index('è©¦åˆæ—¥')['ç´¯ç©å‹ç‚¹']
                    
                    team_weekly_points = team_cumulative_points.reindex(weekly_mondays, method='ffill')
                    weekly_rank_data[team] = team_weekly_points
                
                weekly_rank_data = weekly_rank_data.fillna(0)

                weekly_rank_df_rank = weekly_rank_data.rank(axis=1, ascending=False, method='min')
                
                fig, ax = plt.subplots(figsize=(12, 8))
                
                all_plotted_rank_data = []
                
                for team in selected_teams_rank:
                    if team in weekly_rank_df_rank.columns:
                        team_rank_data = weekly_rank_df_rank[team].dropna()
                        ax.plot(team_rank_data.index, team_rank_data.values, marker='o', linestyle='-', label=team)
                        all_plotted_rank_data.append(team_rank_data)

                if all_plotted_rank_data:
                    num_teams_in_league = len(all_teams_in_selected_league)
                    ax.set_yticks(range(1, num_teams_in_league + 1)) 
                    ax.invert_yaxis()
                    ax.set_ylim(num_teams_in_league + 1, 0)
                else:
                    st.warning("é¸æŠã—ãŸãƒãƒ¼ãƒ ã®é †ä½ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                    st.stop()
                
                ax.set_title(f'{selected_league_rank} é †ä½å¤‰å‹• ({st.session_state.current_year}å¹´ æ¯é€±æœˆæ›œæ—¥æ™‚ç‚¹)')
                ax.set_xlabel('è©¦åˆæ—¥ (æ¯é€±æœˆæ›œæ—¥)')
                ax.set_ylabel('é †ä½')
                ax.grid(True)
                
                ax.legend(title="ãƒãƒ¼ãƒ ", loc='best')
                
                ax.xaxis.set_major_locator(mdates.DayLocator(interval=14))
                ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))
                
                plt.xticks(rotation=90)
                plt.tight_layout()
                
                st.pyplot(fig)
        else:
            st.error("æ—¥ç¨‹è¡¨ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€é †ä½å¤‰å‹•ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

except Exception as e:
    logging.critical(f"--- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æœªè£œè¶³ã®è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {e} ---", exc_info=True)
    st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
