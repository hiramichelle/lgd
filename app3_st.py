import streamlit as st
import pandas as pd
import logging
import requests
import time # <-- ãƒªãƒˆãƒ©ã‚¤æ™‚ã®å¾…æ©Ÿå‡¦ç†ã®ãŸã‚ã«timeãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm # ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’æ‰‹å‹•ã§è¡Œã†å ´åˆã«å‚™ãˆã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¯æ®‹ã—ã¦ãŠã
import matplotlib.ticker as ticker
import matplotlib.dates as mdates

# --- æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š ---
# ç¾çŠ¶ã§ã¯æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®šã¯è¡Œã‚ãšã€matplotlibã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
try:
    plt.rcParams['axes.unicode_minus'] = False # ãƒã‚¤ãƒŠã‚¹è¨˜å·ã®è¡¨ç¤ºã¯ç¶­æŒ
    st.info("â€»ã‚°ãƒ©ãƒ•ã®æ—¥æœ¬èªã¯æ–‡å­—åŒ–ã‘ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
except Exception as e:
    st.warning(f"ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    plt.rcParams['axes.unicode_minus'] = False

# --- ãƒ­ã‚°è¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
)

logging.info("--- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ ---")

# --- å®šæ•°è¨­å®š ---
MAX_RETRIES = 3
RETRY_DELAY = 5 # ãƒªãƒˆãƒ©ã‚¤æ™‚ã®å¾…æ©Ÿæ™‚é–“ (ç§’)

# --------------------------------------------------------------------------
# Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•° (ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ã‚’è¿½åŠ )
# --------------------------------------------------------------------------
@st.cache_data
def scrape_ranking_data(url):
    """
    Jãƒªãƒ¼ã‚°å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰é †ä½è¡¨ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹é–¢æ•°ã€‚ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰
    """
    logging.info(f"scrape_ranking_data: URL {url} ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ã€‚")
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            # requests.get ã§HTMLã‚’å–å¾—ã—ã€ãã‚Œã‚’pandasã«æ¸¡ã™
            response = requests.get(url, headers=headers, timeout=30) 
            response.raise_for_status() # HTTPã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°ä¾‹å¤–ã‚’ç™ºç”Ÿã•ã›ã‚‹
            
            # é †ä½è¡¨ã®ãƒ˜ãƒƒãƒ€ãƒ¼ãŒæ­£ã—ãå–ã‚Œãªã„å ´åˆãŒã‚ã‚‹ãŸã‚ã€'é †ä½'ã‚’å«ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãƒãƒƒãƒ
            dfs = pd.read_html(response.text, flavor='lxml', header=0, match='é †ä½') 
            
            if not dfs:
                logging.warning(f"read_htmlãŒãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚URL: {url}")
                return None
            
            df = dfs[0]
            logging.info(f"é †ä½è¡¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆåŠŸ (è©¦è¡Œ {attempt} å›ç›®)ã€‚DataFrameã®å½¢çŠ¶: {df.shape}")
            if 'å‚™è€ƒ' in df.columns:
                df = df.drop(columns=['å‚™è€ƒ'])
            return df
        
        except requests.exceptions.HTTPError as errh:
            # 5xx ã‚¨ãƒ©ãƒ¼ã®å ´åˆã®ã¿ãƒªãƒˆãƒ©ã‚¤ã‚’è©¦ã¿ã‚‹
            if 500 <= errh.response.status_code < 600 and attempt < MAX_RETRIES:
                logging.warning(f"HTTP 5xx ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {errh}ã€‚{RETRY_DELAY}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™ (è©¦è¡Œ {attempt}/{MAX_RETRIES})ã€‚")
                time.sleep(RETRY_DELAY)
                continue
            logging.error(f"HTTPã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ (æœ€çµ‚è©¦è¡Œ): {errh}")
            return None
        
        except requests.exceptions.RequestException as err:
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚„æ¥ç¶šã‚¨ãƒ©ãƒ¼ã®å ´åˆ
            if attempt < MAX_RETRIES:
                logging.warning(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {err}ã€‚{RETRY_DELAY}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™ (è©¦è¡Œ {attempt}/{MAX_RETRIES})ã€‚")
                time.sleep(RETRY_DELAY)
                continue
            logging.error(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ (æœ€çµ‚è©¦è¡Œ): {err}")
            return None
            
        except Exception as e:
            logging.error(f"é †ä½è¡¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
            return None
    
    return None # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’è¶…ãˆãŸå ´åˆ

@st.cache_data
def scrape_schedule_data(url):
    """
    Jãƒªãƒ¼ã‚°å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰æ—¥ç¨‹è¡¨ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹é–¢æ•°ã€‚ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰
    """
    logging.info(f"scrape_schedule_data: URL {url} ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ã€‚")
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

    for attempt in range(1, MAX_RETRIES + 1):
        try:
            # requests.get ã§HTMLã‚’å–å¾—ã—ã€ãã‚Œã‚’pandasã«æ¸¡ã™
            response = requests.get(url, headers=headers, timeout=30) # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¯30ç§’ã®ã¾ã¾
            response.raise_for_status() # HTTPã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°ä¾‹å¤–ã‚’ç™ºç”Ÿã•ã›ã‚‹
            
            dfs = pd.read_html(response.text, flavor='lxml', header=0, match='è©¦åˆæ—¥') 
            
            if not dfs:
                logging.warning(f"read_htmlãŒãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚URL: {url}")
                return None
                
            df = dfs[0]
            logging.info(f"æ—¥ç¨‹è¡¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆåŠŸ (è©¦è¡Œ {attempt} å›ç›®)ã€‚DataFrameã®å½¢çŠ¶: {df.shape}, ã‚«ãƒ©ãƒ æ•°: {len(df.columns)}")
            
            expected_cols = ['å¤§ä¼š', 'è©¦åˆæ—¥', 'ã‚­ãƒƒã‚¯ã‚ªãƒ•', 'ã‚¹ã‚¿ã‚¸ã‚¢ãƒ ', 'ãƒ›ãƒ¼ãƒ ', 'ã‚¹ã‚³ã‚¢', 'ã‚¢ã‚¦ã‚§ã‚¤', 'ãƒ†ãƒ¬ãƒ“ä¸­ç¶™']
            cols_to_keep = [col for col in expected_cols if col in df.columns]
            
            if len(cols_to_keep) < 5:
                logging.error("æŠ½å‡ºã§ããŸåˆ—æ•°ãŒå°‘ãªã™ãã¾ã™ã€‚ã‚µã‚¤ãƒˆã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒå¤§å¹…ã«å¤‰æ›´ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                return None
                
            df = df[cols_to_keep]
            return df
            
        except requests.exceptions.HTTPError as errh:
            # 5xx ã‚¨ãƒ©ãƒ¼ã®å ´åˆã®ã¿ãƒªãƒˆãƒ©ã‚¤ã‚’è©¦ã¿ã‚‹
            if 500 <= errh.response.status_code < 600 and attempt < MAX_RETRIES:
                logging.warning(f"HTTP 5xx ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {errh}ã€‚{RETRY_DELAY}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™ (è©¦è¡Œ {attempt}/{MAX_RETRIES})ã€‚")
                time.sleep(RETRY_DELAY)
                continue
            logging.error(f"HTTPã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ (æœ€çµ‚è©¦è¡Œ): {errh}")
            return None
        
        except requests.exceptions.RequestException as err:
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚„æ¥ç¶šã‚¨ãƒ©ãƒ¼ã®å ´åˆ
            if attempt < MAX_RETRIES:
                logging.warning(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {err}ã€‚{RETRY_DELAY}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™ (è©¦è¡Œ {attempt}/{MAX_RETRIES})ã€‚")
                time.sleep(RETRY_DELAY)
                continue
            logging.error(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ (æœ€çµ‚è©¦è¡Œ): {err}")
            return None
            
        except Exception as e:
            logging.error(f"æ—¥ç¨‹è¡¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
            return None
            
    return None # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’è¶…ãˆãŸå ´åˆ

# --------------------------------------------------------------------------
# ãƒ‡ãƒ¼ã‚¿åŠ å·¥é–¢æ•° (å¤‰æ›´ãªã—)
# --------------------------------------------------------------------------
@st.cache_data
def create_point_aggregate_df(schedule_df):
    """æ—¥ç¨‹è¡¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€ãƒãƒ¼ãƒ ã”ã¨ã®è©¦åˆçµæœã‚’é›†è¨ˆã™ã‚‹DataFrameã‚’ä½œæˆ"""
    if schedule_df is None or schedule_df.empty:
        return pd.DataFrame()

    df = schedule_df.copy()
    
    # ã‚¹ã‚³ã‚¢ãŒã€Œæ•°å­—-æ•°å­—ã€ã®å½¢å¼ã§ãªã„è¡Œã‚’é™¤å¤–
    df = df[df['ã‚¹ã‚³ã‚¢'].str.contains('^\d+-\d+$', na=False)]
    if df.empty:
        return pd.DataFrame()
    
    df[['å¾—ç‚¹H', 'å¾—ç‚¹A']] = df['ã‚¹ã‚³ã‚¢'].str.split('-', expand=True).astype(int)

    # è©¦åˆæ—¥ã®å‰å‡¦ç†
    df['è©¦åˆæ—¥'] = df['è©¦åˆæ—¥'].str.replace(r'\(.+\)', '', regex=True)
    df['è©¦åˆæ—¥'] = df['è©¦åˆæ—¥'].apply(lambda x: '20' + x if not x.startswith('20') else x)
    try:
        df['è©¦åˆæ—¥'] = pd.to_datetime(df['è©¦åˆæ—¥'], format='%Y/%m/%d')
    except ValueError as e:
        logging.error(f"æ—¥ä»˜å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame()
    
    # ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ã®ãƒ‡ãƒ¼ã‚¿é›†è¨ˆ
    home_df = df.rename(columns={'ãƒ›ãƒ¼ãƒ ': 'ãƒãƒ¼ãƒ ', 'ã‚¢ã‚¦ã‚§ã‚¤': 'ç›¸æ‰‹', 'å¾—ç‚¹H': 'å¾—ç‚¹', 'å¾—ç‚¹A': 'å¤±ç‚¹'})
    home_df['å¾—å¤±å·®'] = home_df['å¾—ç‚¹'] - home_df['å¤±ç‚¹']
    home_df['å‹æ•—'] = home_df.apply(lambda row: 'å‹' if row['å¾—ç‚¹'] > row['å¤±ç‚¹'] else ('åˆ†' if row['å¾—ç‚¹'] == row['å¤±ç‚¹'] else 'æ•—'), axis=1)
    home_df['å‹ç‚¹'] = home_df.apply(lambda row: 3 if row['å‹æ•—'] == 'å‹' else (1 if row['å‹æ•—'] == 'åˆ†' else 0), axis=1)
    home_df['å¯¾æˆ¦ç›¸æ‰‹'] = home_df['ç›¸æ‰‹']
    home_df = home_df[['å¤§ä¼š', 'è©¦åˆæ—¥', 'ãƒãƒ¼ãƒ ', 'å¯¾æˆ¦ç›¸æ‰‹', 'å‹æ•—', 'å¾—ç‚¹', 'å¤±ç‚¹', 'å¾—å¤±å·®', 'å‹ç‚¹']]

    # ã‚¢ã‚¦ã‚§ã‚¤ãƒãƒ¼ãƒ ã®ãƒ‡ãƒ¼ã‚¿é›†è¨ˆ
    away_df = df.rename(columns={'ã‚¢ã‚¦ã‚§ã‚¤': 'ãƒãƒ¼ãƒ ', 'ãƒ›ãƒ¼ãƒ ': 'ç›¸æ‰‹', 'å¾—ç‚¹A': 'å¾—ç‚¹', 'å¾—ç‚¹H': 'å¤±ç‚¹'})
    away_df['å¾—å¤±å·®'] = away_df['å¾—ç‚¹'] - away_df['å¤±ç‚¹']
    away_df['å‹æ•—'] = away_df.apply(lambda row: 'å‹' if row['å¾—ç‚¹'] > row['å¤±ç‚¹'] else ('åˆ†' if row['å¾—ç‚¹'] == row['å¤±ç‚¹'] else 'æ•—'), axis=1)
    away_df['å‹ç‚¹'] = away_df.apply(lambda row: 3 if row['å‹æ•—'] == 'å‹' else (1 if row['å‹æ•—'] == 'åˆ†' else 0), axis=1)
    away_df['å¯¾æˆ¦ç›¸æ‰‹'] = away_df['ç›¸æ‰‹']
    away_df = away_df[['å¤§ä¼š', 'è©¦åˆæ—¥', 'ãƒãƒ¼ãƒ ', 'å¯¾æˆ¦ç›¸æ‰‹', 'å‹æ•—', 'å¾—ç‚¹', 'å¤±ç‚¹', 'å¾—å¤±å·®', 'å‹ç‚¹']]

    # ãƒ›ãƒ¼ãƒ ã¨ã‚¢ã‚¦ã‚§ã‚¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    pointaggregate_df = pd.concat([home_df, away_df], ignore_index=True)

    # è©¦åˆæ—¥ã§ã‚½ãƒ¼ãƒˆã—ã€ç´¯ç©å‹ç‚¹ã‚’è¨ˆç®—
    pointaggregate_df = pointaggregate_df.sort_values(by=['è©¦åˆæ—¥'], ascending=True)
    pointaggregate_df['ç´¯ç©å‹ç‚¹'] = pointaggregate_df.groupby(['ãƒãƒ¼ãƒ '])['å‹ç‚¹'].cumsum()

    return pointaggregate_df


# --------------------------------------------------------------------------
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æœ¬ä½“
# --------------------------------------------------------------------------
st.title('ğŸ“Š Jãƒªãƒ¼ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ“ãƒ¥ãƒ¼ã‚¢')

# --- ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰ã¨åˆæœŸå‡¦ç†ã‚’ st.spinner ã§ãƒ©ãƒƒãƒ—ã™ã‚‹ ---
try:
    with st.spinner("Jãƒªãƒ¼ã‚°å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ãƒ»å‡¦ç†ä¸­ã§ã™ã€‚åˆå›ãƒ­ãƒ¼ãƒ‰ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ (1ã€œ2åˆ†ç¨‹åº¦)..."):
        
        # --- ãƒ‡ãƒ¼ã‚¿ã®å–å¾— ---
        current_year = 2024 # ãƒ‡ãƒ¼ã‚¿å–å¾—å¹´ã‚’2024ã«è¨­å®š
        ranking_urls = {
            'J1': f'https://data.j-league.or.jp/SFRT01/?competitionSectionIdLabel=%E6%9C%80%E6%96%B0%E7%AF%80&competitionIdLabel=%E6%98%8E%E6%B2%BB%E5%AE%89%E7%94%B0%EF%BC%AA%EF%BC%91%E3%83%AA%E3%83%BC%E3%82%B0&yearIdLabel={current_year}&yearId={current_year}&competitionId=651&competitionSectionId=0&search=search',
            'J2': f'https://data.j-league.or.jp/SFRT01/?competitionSectionIdLabel=%E6%9C%80%E6%96%B0%E7%AF%80&competitionIdLabel=%E6%98%8E%E6%B2%BB%E7%94%B0%EF%BC%AA%EF%BC%92%E3%83%AA%E3%83%BC%E3%82%B0&yearIdLabel={current_year}&yearId={current_year}&competitionId=655&competitionSectionId=0&search=search',
            'J3': f'https://data.j-league.or.jp/SFRT01/?competitionSectionIdLabel=%E6%9C%80%E6%96%B0%E7%AF%80&competitionIdLabel=%E6%98%8E%E6%B2%BB%E7%94%B0%EF%BC%AA%EF%BC%93%E3%83%AA%E3%83%BC%E3%82%B0&yearIdLabel={current_year}&yearId={current_year}&competitionId=657&competitionSectionId=0&search=search'
        }
        schedule_url = f'https://data.j-league.or.jp/SFMS01/search?competition_years={current_year}&competition_frame_ids=1&competition_frame_ids=2&competition_frame_ids=3&tv_relay_station_name='

        # é †ä½è¡¨ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¨çµåˆ (4ã¤ã®ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ)
        ranking_dfs = {league: scrape_ranking_data(url) for league, url in ranking_urls.items()}
        for league, df in ranking_dfs.items():
            if df is not None: df['å¤§ä¼š'] = league
        combined_ranking_df = pd.concat([df for df in ranking_dfs.values() if df is not None], ignore_index=True)

        # æ—¥ç¨‹è¡¨ãƒ‡ãƒ¼ã‚¿ã®å–å¾— (1ã¤ã®ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ)
        schedule_df = scrape_schedule_data(schedule_url)

        # ç´¯ç©å‹ç‚¹ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä½œæˆ
        pointaggregate_df = create_point_aggregate_df(schedule_df)

except Exception as e:
    logging.critical(f"--- ãƒ‡ãƒ¼ã‚¿ã®åˆæœŸãƒ­ãƒ¼ãƒ‰ä¸­ã«è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {e} ---", exc_info=True)
    st.error("ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¾ãŸã¯åˆæœŸå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚æ™‚é–“ã‚’ç½®ã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
    st.stop() # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã«ä»¥é™ã®å‡¦ç†ã‚’åœæ­¢

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«å¤§ä¼šãƒ»ãƒãƒ¼ãƒ é¸æŠã‚’ä¸Šä½ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«é…ç½® ---
with st.sidebar:
    st.header("ã‚¹ãƒ†ãƒƒãƒ— 1: å¤§ä¼šã¨ãƒãƒ¼ãƒ é¸æŠ")

    # 1. å¤§ä¼šé¸æŠ (ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã¨æ—¥ç¨‹è¡¨ã®ä¸¡æ–¹ã‹ã‚‰å­˜åœ¨ã™ã‚‹å¤§ä¼šã‚’æŠ½å‡º)
    all_available_leagues = set()
    if not combined_ranking_df.empty:
        all_available_leagues.update(combined_ranking_df['å¤§ä¼š'].unique())
    if schedule_df is not None and not schedule_df.empty:
        all_available_leagues.update(schedule_df['å¤§ä¼š'].unique())
        
    if not all_available_leagues:
        st.error("ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å¤§ä¼šé¸æŠã«é€²ã‚ã¾ã›ã‚“ã€‚")
        st.stop()
    
    league_options = sorted(list(all_available_leagues))
    selected_league = st.selectbox(
        'è¡¨ç¤ºã—ãŸã„å¤§ä¼šã‚’é¸æŠã—ã¦ãã ã•ã„:', 
        league_options, 
        key='global_league_selectbox'
    )

    # 2. ãƒãƒ¼ãƒ é¸æŠ (é¸æŠã•ã‚ŒãŸå¤§ä¼šã®æ—¥ç¨‹è¡¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒãƒ¼ãƒ ã‚’æŠ½å‡º)
    team_options = []
    selected_team = None
    if schedule_df is not None and not schedule_df.empty:
        filtered_by_league_schedule = schedule_df[schedule_df['å¤§ä¼š'] == selected_league]
        if not filtered_by_league_schedule.empty:
            all_teams_in_league = pd.concat([filtered_by_league_schedule['ãƒ›ãƒ¼ãƒ '], filtered_by_league_schedule['ã‚¢ã‚¦ã‚§ã‚¤']]).unique()
            team_options = sorted(all_teams_in_league)
        
            if team_options:
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå‰ã«é¸æŠã—ãŸãƒãƒ¼ãƒ ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ã™ã‚‹
                default_team_index = 0
                if 'global_team_selectbox' in st.session_state and st.session_state.global_team_selectbox in team_options:
                    default_team_index = team_options.index(st.session_state.global_team_selectbox)

                selected_team = st.selectbox(
                    'åŸºæº–ã¨ãªã‚‹ãƒãƒ¼ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„:', 
                    team_options, 
                    index=default_team_index,
                    key='global_team_selectbox'
                )
        
    if not team_options:
        st.warning("ã“ã®å¤§ä¼šã®æ—¥ç¨‹ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€ãƒãƒ¼ãƒ é¸æŠã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
    

    st.header("ã‚¹ãƒ†ãƒƒãƒ— 2: è¡¨ç¤ºãƒ‡ãƒ¼ã‚¿é¸æŠ")
    data_type = st.radio(
        "è¡¨ç¤ºã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠã—ã¦ãã ã•ã„:", 
        ("é †ä½è¡¨", "æ—¥ç¨‹è¡¨", "ç›´è¿‘5è©¦åˆ", "é †ä½å¤‰å‹•ã‚°ãƒ©ãƒ•")
    )

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢ã®è¡¨ç¤ºãƒ­ã‚¸ãƒƒã‚¯ ---
if data_type == "é †ä½è¡¨":
    st.header(f"{selected_league} é †ä½è¡¨")
    if not combined_ranking_df.empty:
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§é¸æŠã•ã‚ŒãŸå¤§ä¼šã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_df = combined_ranking_df[combined_ranking_df['å¤§ä¼š'] == selected_league].drop(columns=['å¤§ä¼š'])
        st.dataframe(filtered_df)
    else:
        st.error("é †ä½è¡¨ãƒ‡ãƒ¼ã‚¿ãŒæ­£å¸¸ã«å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

elif data_type == "æ—¥ç¨‹è¡¨":
    st.header(f"{selected_league} {selected_team if selected_team else ''} è©¦åˆæ—¥ç¨‹")
    if schedule_df is not None and not schedule_df.empty and selected_team:
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§é¸æŠã•ã‚ŒãŸå¤§ä¼šã¨ãƒãƒ¼ãƒ ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        team_filter = (schedule_df['ãƒ›ãƒ¼ãƒ '] == selected_team) | (schedule_df['ã‚¢ã‚¦ã‚§ã‚¤'] == selected_team)
        final_filtered_df = schedule_df[(schedule_df['å¤§ä¼š'] == selected_league) & team_filter]
        st.dataframe(final_filtered_df)
    elif selected_team is None:
         st.warning("ãƒãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€æ—¥ç¨‹è¡¨ã‚’è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")
    else:
        st.error("æ—¥ç¨‹è¡¨ãƒ‡ãƒ¼ã‚¿ãŒæ­£å¸¸ã«å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

elif data_type == "ç›´è¿‘5è©¦åˆ":
    st.header(f"{selected_league} {selected_team if selected_team else ''} ç›´è¿‘5è©¦åˆçµæœ")
    if not pointaggregate_df.empty and selected_team:
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§é¸æŠã•ã‚ŒãŸå¤§ä¼šã¨ãƒãƒ¼ãƒ ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        team_results = pointaggregate_df[(pointaggregate_df['å¤§ä¼š'] == selected_league) & (pointaggregate_df['ãƒãƒ¼ãƒ '] == selected_team)]
        
        if team_results.empty:
             st.info(f"ãƒãƒ¼ãƒ  {selected_team} ã®è©¦åˆçµæœãƒ‡ãƒ¼ã‚¿ãŒã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            # æœ€æ–°ã®5è©¦åˆã‚’å–å¾—
            recent_5_games = team_results.sort_values(by='è©¦åˆæ—¥', ascending=False).head(5)
            recent_5_games = recent_5_games.sort_values(by='è©¦åˆæ—¥', ascending=True) # è¡¨ç¤ºã®ãŸã‚ã«å†åº¦æ˜‡é †ã«ã‚½ãƒ¼ãƒˆ
            recent_5_games['è©¦åˆæ—¥'] = recent_5_games['è©¦åˆæ—¥'].dt.strftime('%y%m%d')
            
            st.dataframe(recent_5_games[['è©¦åˆæ—¥', 'å¯¾æˆ¦ç›¸æ‰‹', 'å‹æ•—', 'å¾—ç‚¹', 'å¤±ç‚¹', 'å‹ç‚¹']])
    elif selected_team is None:
         st.warning("ãƒãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€ç›´è¿‘5è©¦åˆã®çµæœã‚’è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")
    else:
        st.error("æ—¥ç¨‹è¡¨ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€ç›´è¿‘5è©¦åˆã®é›†è¨ˆãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

elif data_type == "é †ä½å¤‰å‹•ã‚°ãƒ©ãƒ•":
    st.header(f"{selected_league} é †ä½å¤‰å‹•ã‚°ãƒ©ãƒ•")
    if not pointaggregate_df.empty:
        
        # --- ã‚°ãƒ©ãƒ•è¡¨ç¤ºç”¨ã®ãƒãƒ¼ãƒ ãƒãƒ«ãƒã‚»ãƒ¬ã‚¯ãƒˆã‚’ã“ã“ã§å®šç¾© ---
        filtered_df_rank = pointaggregate_df[pointaggregate_df['å¤§ä¼š'] == selected_league]
        team_options_rank = sorted(filtered_df_rank['ãƒãƒ¼ãƒ '].unique())
        
        # ã‚°ãƒ©ãƒ•ç”¨ã®ãƒãƒ¼ãƒ é¸æŠï¼ˆè¤‡æ•°é¸æŠï¼‰
        default_teams = []
        if selected_team in team_options_rank:
            default_teams = [selected_team]
        elif team_options_rank:
             default_teams = team_options_rank[:1]

        selected_teams_rank = st.multiselect(
            'ã‚°ãƒ©ãƒ•ã«è¡¨ç¤ºã™ã‚‹ãƒãƒ¼ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„ (è¤‡æ•°é¸æŠå¯):', 
            team_options_rank, 
            default=default_teams, 
            key='rank_team_multiselect'
        )
        # --- ã‚°ãƒ©ãƒ•è¡¨ç¤ºç”¨ã®ãƒãƒ¼ãƒ ãƒãƒ«ãƒã‚»ãƒ¬ã‚¯ãƒˆã“ã“ã¾ã§ ---

        if not selected_teams_rank:
            st.warning("è¡¨ç¤ºã™ã‚‹ãƒãƒ¼ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        else:
            # é †ä½ç®—å‡ºãƒ­ã‚¸ãƒƒã‚¯ã¯å¤‰æ›´ãªã—
            min_date = filtered_df_rank['è©¦åˆæ—¥'].min()
            max_date = filtered_df_rank['è©¦åˆæ—¥'].max()
            
            # æœ€åˆã®æœˆæ›œæ—¥ã‚’è¨ˆç®—
            start_monday_candidate = min_date - pd.to_timedelta(min_date.weekday(), unit='D')
            if start_monday_candidate < min_date:
                start_monday = start_monday_candidate + pd.to_timedelta(7, unit='D')
            else:
                start_monday = start_monday_candidate
            
            # æ¯é€±æœˆæ›œæ—¥ã®æ—¥ä»˜ç¯„å›²ã‚’ç”Ÿæˆ
            weekly_mondays = pd.date_range(start=start_monday, end=max_date + pd.to_timedelta(7, unit='D'), freq='W-MON')
            
            all_teams_in_selected_league = filtered_df_rank['ãƒãƒ¼ãƒ '].unique()
            
            weekly_rank_data = pd.DataFrame(index=weekly_mondays)

            for team in all_teams_in_selected_league: 
                team_cumulative_points = filtered_df_rank[
                    filtered_df_rank['ãƒãƒ¼ãƒ '] == team
                ].set_index('è©¦åˆæ—¥')['ç´¯ç©å‹ç‚¹']
                
                team_weekly_points = team_cumulative_points.reindex(weekly_mondays, method='ffill')
                weekly_rank_data[team] = team_weekly_points
            
            weekly_rank_data = weekly_rank_data.fillna(0)

            # é€±ã”ã¨ã®é †ä½ã‚’è¨ˆç®—
            weekly_rank_df_rank = weekly_rank_data.rank(axis=1, ascending=False, method='min')
            
            # --- ã‚°ãƒ©ãƒ•æç”» ---
            fig, ax = plt.subplots(figsize=(12, 8))
            
            all_plotted_rank_data = []
            
            for team in selected_teams_rank:
                if team in weekly_rank_df_rank.columns:
                    team_rank_data = weekly_rank_df_rank[team].dropna()
                    ax.plot(team_rank_data.index, team_rank_data.values, marker='o', linestyle='-', label=team)
                    all_plotted_rank_data.append(team_rank_data)

            if all_plotted_rank_data:
                num_teams_in_league = len(all_teams_in_selected_league)
                
                ax.set_yticks(range(1, num_teams_in_league + 1)) 
                ax.invert_yaxis() # é †ä½ã¯å°ã•ã„æ–¹ãŒä¸Šã«æ¥ã‚‹ã‚ˆã†ã«åè»¢
                
                # yè»¸ã®ç¯„å›²ã‚’èª¿æ•´ï¼ˆé †ä½ã®æœ€å¤§å€¤ã¾ã§ï¼‰
                ax.set_ylim(num_teams_in_league + 1, 0)
            else:
                st.warning("é¸æŠã—ãŸãƒãƒ¼ãƒ ã®é †ä½ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                st.stop()
            
            ax.set_title(f'{selected_league} é †ä½å¤‰å‹• (æ¯é€±æœˆæ›œæ—¥æ™‚ç‚¹)')
            ax.set_xlabel('è©¦åˆæ—¥ (æ¯é€±æœˆæ›œæ—¥)')
            ax.set_ylabel('é †ä½')
            ax.grid(True)
            
            ax.legend(title="ãƒãƒ¼ãƒ ", loc='best')
            
            ax.xaxis.set_major_locator(mdates.DayLocator(interval=14)) # 2é€±é–“ã”ã¨ã®ç›®ç››ã‚Š
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d')) # æœˆ/æ—¥ã®å½¢å¼
            
            plt.xticks(rotation=90) # Xè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’ç¸¦ã«
            plt.tight_layout() # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’èª¿æ•´
            
            st.pyplot(fig)
    else:
        st.error("æ—¥ç¨‹è¡¨ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€é †ä½å¤‰å‹•ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
